{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import potrebnih biblioteka"
      ],
      "metadata": {
        "id": "8HXij17oYHTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK468PYZp9Ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ccd410-8edf-4b34-b57a-78971f223846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Priprema podataka"
      ],
      "metadata": {
        "id": "o20wxy7PYMi5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH50zN9Uz5El",
        "outputId": "d19f29c2-1843-4c39-f1d4-0a7964d26bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-2f21ace0f07d>:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  image_paths = image_paths.drop(\"images\",1)\n"
          ]
        }
      ],
      "source": [
        "people = pd.DataFrame()\n",
        "people['name'] = ['Alma_Cemer', 'Angelina_Jolie', 'Brad_Pitt', 'Kate_Winslet', 'Leonardo_DiCaprio', 'Natalie_Portman', 'Nedim_Omeragic',  'Scarlett_Johansson',  'Tom_Cruise', 'Will_Smith']\n",
        "people['images'] = [113, 93, 95, 96, 97, 97, 113, 114, 93, 96]\n",
        "\n",
        "image_paths = people.loc[people.index.repeat(people['images'])]\n",
        "image_paths['image_path'] = 1 + image_paths.groupby('name').cumcount()\n",
        "image_paths['image_path'] = image_paths.image_path.apply(lambda x: '{0:0>4}'.format(x))\n",
        "image_paths['image_path'] = image_paths.name + \"/\" + image_paths.name + \"_\" + image_paths.image_path + \".jpg\"\n",
        "image_paths = image_paths.drop(\"images\",1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Formiranje train i val setova\n"
      ],
      "metadata": {
        "id": "VFrvxpLlYXq5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAr2DvoY1ORo"
      },
      "outputs": [],
      "source": [
        "names_list = image_paths['name'].value_counts().index.tolist()\n",
        "\n",
        "data_frames_train = []\n",
        "data_frames_val = []\n",
        "\n",
        "for i in names_list:\n",
        "  class_samples = image_paths[image_paths['name'] == i]\n",
        "  train_set, val_set = train_test_split(class_samples, test_size=0.2, random_state=42)\n",
        "  data_frames_train.append(train_set)\n",
        "  data_frames_val.append(val_set)\n",
        "\n",
        "multi_train = pd.concat(data_frames_train)\n",
        "multi_train = multi_train.sample(frac = 1)\n",
        "\n",
        "multi_val = pd.concat(data_frames_val)\n",
        "multi_val = multi_val.sample(frac = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Funkcija za kreiranje direktorija"
      ],
      "metadata": {
        "id": "D_gGM_n9Yfea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dFPF5rt2FX_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from shutil import unpack_archive\n",
        "def directory_mover(data,dir_name):\n",
        "    co = 0\n",
        "    for image in data.image_path:\n",
        "        if not os.path.exists(os.path.join('/content/drive/MyDrive/',dir_name)):\n",
        "            shutil.os.mkdir(os.path.join('/content/drive/MyDrive/',dir_name))\n",
        "\n",
        "        data_type = data[data['image_path'] == image]['name']\n",
        "        data_type = str(list(data_type)[0])\n",
        "        if not os.path.exists(os.path.join('/content/drive/MyDrive/',dir_name,data_type)):\n",
        "            shutil.os.mkdir(os.path.join('/content/drive/MyDrive/',dir_name,data_type))\n",
        "        path_from = os.path.join('/content/drive/MyDrive/images_yolo/',image)\n",
        "        path_to = os.path.join('/content/drive/MyDrive/',dir_name,data_type)\n",
        "        shutil.copy(path_from, path_to)\n",
        "        co += 1\n",
        "\n",
        "    print('Moved {} images to {} folder.'.format(co,dir_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Brisanje direktorija sa train i val podacima\n"
      ],
      "metadata": {
        "id": "5XC7SeUGYl7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WuKM2jD24vf"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(\"/content/drive/MyDrive/face_recognition/train\")\n",
        "shutil.rmtree(\"/content/drive/MyDrive/face_recognition/val\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kreiranje direktorija sa train i val podacima"
      ],
      "metadata": {
        "id": "4EMZVB7fYpoa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fithRKcomEAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "08a29784-04a5-4ee5-85d6-32a495b22d1b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f68ec6378d50>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nije potrebno pokretati, oba foldera se nalaze na driveu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdirectory_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train_multi/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdirectory_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val_multi/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5463b4afdb91>\u001b[0m in \u001b[0;36mdirectory_mover\u001b[0;34m(data, dir_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/images_yolo/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpath_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mco\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0m_fastcopy_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPYFILE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#nije potrebno pokretati, oba foldera se nalaze na driveu\n",
        "directory_mover(multi_train,\"train_multi/\")\n",
        "directory_mover(multi_val,\"val_multi/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naš ANN model\n"
      ],
      "metadata": {
        "id": "_ZGHfguQphiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "X_train_flat = []\n",
        "for file in multi_train['image_path']:\n",
        "    path = \"/content/drive/MyDrive/images_yolo/\" + str(file)\n",
        "    im = Image.open(path)\n",
        "    im_array = np.array(im)\n",
        "    X_train_flat.append(im_array.flatten())\n",
        "\n",
        "X_val_flat = []\n",
        "for file in multi_val['image_path']:\n",
        "    path = \"/content/drive/MyDrive/images_yolo/\" + str(file)\n",
        "    im = Image.open(path)\n",
        "    im_array = np.array(im)\n",
        "    X_val_flat.append(im_array.flatten())\n",
        "\n",
        "X_train_flat = np.array(X_train_flat)\n",
        "X_val_flat = np.array(X_val_flat)\n",
        "\n",
        "\n",
        "X_train_flat = X_train_flat.astype('float32') / 255\n",
        "X_val_flat = X_val_flat.astype('float32') / 255\n",
        "label_mapping = {'Alma_Cemer': 0, 'Angelina_Jolie': 1, 'Brad_Pitt': 2, 'Kate_Winslet': 3, 'Leonardo_DiCaprio': 4, 'Natalie_Portman': 5, 'Nedim_Omeragic': 6, 'Scarlett_Johansson': 7, 'Tom_Cruise': 8, 'Will_Smith': 9}\n",
        "y_train_encoded = multi_train['name'].map(label_mapping)\n",
        "y_val_encoded = multi_val['name'].map(label_mapping)\n",
        "\n",
        "num_classes = len(label_mapping)\n",
        "y_train_encoded = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "y_val_encoded = to_categorical(y_val_encoded, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "egCuN1BEe77a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(X_train_flat.shape[1],), activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=7)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('ANN_best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "\n",
        "model.fit(X_train_flat, y_train_encoded, epochs=100, batch_size=16, validation_data = (X_val_flat, y_val_encoded), callbacks=[early_stopping, model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FiGqqfUpscn",
        "outputId": "c537d75c-0f16-4ecd-d21d-b6f1547e868e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 57.5810 - accuracy: 0.1211\n",
            "Epoch 1: val_loss improved from inf to 16.45313, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 29s 553ms/step - loss: 57.5810 - accuracy: 0.1211 - val_loss: 16.4531 - val_accuracy: 0.1117\n",
            "Epoch 2/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 4.9954 - accuracy: 0.1950\n",
            "Epoch 2: val_loss improved from 16.45313 to 3.02834, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 17s 333ms/step - loss: 4.9925 - accuracy: 0.1948 - val_loss: 3.0283 - val_accuracy: 0.1505\n",
            "Epoch 3/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.2896\n",
            "Epoch 3: val_loss did not improve from 3.02834\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 2.2005 - accuracy: 0.2896 - val_loss: 4.1110 - val_accuracy: 0.1699\n",
            "Epoch 4/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.8566 - accuracy: 0.3945\n",
            "Epoch 4: val_loss improved from 3.02834 to 2.49612, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 19s 387ms/step - loss: 1.8566 - accuracy: 0.3945 - val_loss: 2.4961 - val_accuracy: 0.1796\n",
            "Epoch 5/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.6995 - accuracy: 0.4425\n",
            "Epoch 5: val_loss improved from 2.49612 to 2.43050, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 16s 324ms/step - loss: 1.7006 - accuracy: 0.4419 - val_loss: 2.4305 - val_accuracy: 0.2330\n",
            "Epoch 6/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.7740 - accuracy: 0.4062\n",
            "Epoch 6: val_loss improved from 2.43050 to 1.68930, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 16s 317ms/step - loss: 1.7740 - accuracy: 0.4057 - val_loss: 1.6893 - val_accuracy: 0.4320\n",
            "Epoch 7/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.5863 - accuracy: 0.4831\n",
            "Epoch 7: val_loss did not improve from 1.68930\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.5863 - accuracy: 0.4831 - val_loss: 1.8460 - val_accuracy: 0.3981\n",
            "Epoch 8/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.4497 - accuracy: 0.5081\n",
            "Epoch 8: val_loss did not improve from 1.68930\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 1.4497 - accuracy: 0.5081 - val_loss: 1.7236 - val_accuracy: 0.4612\n",
            "Epoch 9/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.2921 - accuracy: 0.5612\n",
            "Epoch 9: val_loss did not improve from 1.68930\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.2915 - accuracy: 0.5618 - val_loss: 1.9900 - val_accuracy: 0.3641\n",
            "Epoch 10/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.2653 - accuracy: 0.5612\n",
            "Epoch 10: val_loss improved from 1.68930 to 1.21242, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 16s 321ms/step - loss: 1.2638 - accuracy: 0.5618 - val_loss: 1.2124 - val_accuracy: 0.5728\n",
            "Epoch 11/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.2421 - accuracy: 0.5838\n",
            "Epoch 11: val_loss did not improve from 1.21242\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.2421 - accuracy: 0.5830 - val_loss: 1.8842 - val_accuracy: 0.4223\n",
            "Epoch 12/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.3726 - accuracy: 0.5605\n",
            "Epoch 12: val_loss did not improve from 1.21242\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.3726 - accuracy: 0.5605 - val_loss: 3.9570 - val_accuracy: 0.1845\n",
            "Epoch 13/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.6715 - accuracy: 0.4913\n",
            "Epoch 13: val_loss did not improve from 1.21242\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 1.6699 - accuracy: 0.4919 - val_loss: 1.3892 - val_accuracy: 0.5146\n",
            "Epoch 14/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.0554 - accuracy: 0.6404\n",
            "Epoch 14: val_loss did not improve from 1.21242\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 1.0554 - accuracy: 0.6404 - val_loss: 1.3371 - val_accuracy: 0.5340\n",
            "Epoch 15/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.1657 - accuracy: 0.6162\n",
            "Epoch 15: val_loss did not improve from 1.21242\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 1.1646 - accuracy: 0.6167 - val_loss: 1.2988 - val_accuracy: 0.5631\n",
            "Epoch 16/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.9197 - accuracy: 0.6816\n",
            "Epoch 16: val_loss improved from 1.21242 to 0.97918, saving model to ANN_best_model.h5\n",
            "51/51 [==============================] - 17s 339ms/step - loss: 0.9197 - accuracy: 0.6816 - val_loss: 0.9792 - val_accuracy: 0.6650\n",
            "Epoch 17/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.7216\n",
            "Epoch 17: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 0.8247 - accuracy: 0.7216 - val_loss: 1.0091 - val_accuracy: 0.6990\n",
            "Epoch 18/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.8133 - accuracy: 0.7400\n",
            "Epoch 18: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 0.8132 - accuracy: 0.7403 - val_loss: 1.6082 - val_accuracy: 0.4806\n",
            "Epoch 19/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.1746 - accuracy: 0.6042\n",
            "Epoch 19: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 1.1746 - accuracy: 0.6042 - val_loss: 1.5034 - val_accuracy: 0.4903\n",
            "Epoch 20/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.0905 - accuracy: 0.6162\n",
            "Epoch 20: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.0896 - accuracy: 0.6167 - val_loss: 1.2672 - val_accuracy: 0.5971\n",
            "Epoch 21/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.7289 - accuracy: 0.7578\n",
            "Epoch 21: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 0.7289 - accuracy: 0.7578 - val_loss: 1.3699 - val_accuracy: 0.5534\n",
            "Epoch 22/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.9350 - accuracy: 0.6667\n",
            "Epoch 22: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 0.9350 - accuracy: 0.6667 - val_loss: 1.1530 - val_accuracy: 0.5922\n",
            "Epoch 23/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.8147 - accuracy: 0.7079\n",
            "Epoch 23: val_loss did not improve from 0.97918\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 0.8147 - accuracy: 0.7079 - val_loss: 1.1319 - val_accuracy: 0.6019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ec0902e3490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naš CNN model"
      ],
      "metadata": {
        "id": "phyYYzjXYt_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4UGghr73RcO",
        "outputId": "7e04bf64-dd97-44b1-c110-eacb350a0128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 10 classes.\n",
            "Found 206 images belonging to 10 classes.\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.3085 - accuracy: 0.0975\n",
            "Epoch 1: val_loss improved from inf to 2.29511, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 339ms/step - loss: 2.3085 - accuracy: 0.0975 - val_loss: 2.2951 - val_accuracy: 0.1117\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2974 - accuracy: 0.1063\n",
            "Epoch 2: val_loss improved from 2.29511 to 2.28492, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 299ms/step - loss: 2.2974 - accuracy: 0.1063 - val_loss: 2.2849 - val_accuracy: 0.1117\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2844 - accuracy: 0.1375\n",
            "Epoch 3: val_loss improved from 2.28492 to 2.26720, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 307ms/step - loss: 2.2844 - accuracy: 0.1375 - val_loss: 2.2672 - val_accuracy: 0.1893\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2702 - accuracy: 0.1525\n",
            "Epoch 4: val_loss improved from 2.26720 to 2.24155, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 379ms/step - loss: 2.2702 - accuracy: 0.1525 - val_loss: 2.2415 - val_accuracy: 0.2573\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2257 - accuracy: 0.2200\n",
            "Epoch 5: val_loss improved from 2.24155 to 2.14519, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 306ms/step - loss: 2.2257 - accuracy: 0.2200 - val_loss: 2.1452 - val_accuracy: 0.3350\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.1435 - accuracy: 0.2550\n",
            "Epoch 6: val_loss improved from 2.14519 to 2.02751, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 316ms/step - loss: 2.1435 - accuracy: 0.2550 - val_loss: 2.0275 - val_accuracy: 0.3301\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.0458 - accuracy: 0.3150\n",
            "Epoch 7: val_loss improved from 2.02751 to 1.89462, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 375ms/step - loss: 2.0458 - accuracy: 0.3150 - val_loss: 1.8946 - val_accuracy: 0.3981\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.9089 - accuracy: 0.3475\n",
            "Epoch 8: val_loss improved from 1.89462 to 1.76250, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 305ms/step - loss: 1.9089 - accuracy: 0.3475 - val_loss: 1.7625 - val_accuracy: 0.3981\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.8183 - accuracy: 0.3650\n",
            "Epoch 9: val_loss improved from 1.76250 to 1.58387, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 360ms/step - loss: 1.8183 - accuracy: 0.3650 - val_loss: 1.5839 - val_accuracy: 0.4417\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.7083 - accuracy: 0.4125\n",
            "Epoch 10: val_loss improved from 1.58387 to 1.52700, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 361ms/step - loss: 1.7083 - accuracy: 0.4125 - val_loss: 1.5270 - val_accuracy: 0.4709\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.6169 - accuracy: 0.4325\n",
            "Epoch 11: val_loss improved from 1.52700 to 1.50490, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 374ms/step - loss: 1.6169 - accuracy: 0.4325 - val_loss: 1.5049 - val_accuracy: 0.4806\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.5149 - accuracy: 0.4750\n",
            "Epoch 12: val_loss improved from 1.50490 to 1.31939, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 376ms/step - loss: 1.5149 - accuracy: 0.4750 - val_loss: 1.3194 - val_accuracy: 0.5291\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.4103 - accuracy: 0.5138\n",
            "Epoch 13: val_loss improved from 1.31939 to 1.20089, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 352ms/step - loss: 1.4103 - accuracy: 0.5138 - val_loss: 1.2009 - val_accuracy: 0.6068\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.3636 - accuracy: 0.5288\n",
            "Epoch 14: val_loss improved from 1.20089 to 1.15451, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 317ms/step - loss: 1.3636 - accuracy: 0.5288 - val_loss: 1.1545 - val_accuracy: 0.6068\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2931 - accuracy: 0.5537\n",
            "Epoch 15: val_loss improved from 1.15451 to 1.10011, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 353ms/step - loss: 1.2931 - accuracy: 0.5537 - val_loss: 1.1001 - val_accuracy: 0.6699\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2134 - accuracy: 0.5987\n",
            "Epoch 16: val_loss improved from 1.10011 to 1.08139, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 17s 337ms/step - loss: 1.2134 - accuracy: 0.5987 - val_loss: 1.0814 - val_accuracy: 0.6262\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.1783 - accuracy: 0.6075\n",
            "Epoch 17: val_loss improved from 1.08139 to 1.01377, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 20s 402ms/step - loss: 1.1783 - accuracy: 0.6075 - val_loss: 1.0138 - val_accuracy: 0.6942\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.1053 - accuracy: 0.6363\n",
            "Epoch 18: val_loss improved from 1.01377 to 0.98028, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 17s 345ms/step - loss: 1.1053 - accuracy: 0.6363 - val_loss: 0.9803 - val_accuracy: 0.6990\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0956 - accuracy: 0.6225\n",
            "Epoch 19: val_loss improved from 0.98028 to 0.92501, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 341ms/step - loss: 1.0956 - accuracy: 0.6225 - val_loss: 0.9250 - val_accuracy: 0.6942\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0637 - accuracy: 0.6338\n",
            "Epoch 20: val_loss did not improve from 0.92501\n",
            "50/50 [==============================] - 13s 268ms/step - loss: 1.0637 - accuracy: 0.6338 - val_loss: 0.9408 - val_accuracy: 0.6893\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9822 - accuracy: 0.6787\n",
            "Epoch 21: val_loss improved from 0.92501 to 0.88770, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 374ms/step - loss: 0.9822 - accuracy: 0.6787 - val_loss: 0.8877 - val_accuracy: 0.6990\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9949 - accuracy: 0.6662\n",
            "Epoch 22: val_loss improved from 0.88770 to 0.87275, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 20s 395ms/step - loss: 0.9949 - accuracy: 0.6662 - val_loss: 0.8728 - val_accuracy: 0.7621\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9583 - accuracy: 0.6762\n",
            "Epoch 23: val_loss improved from 0.87275 to 0.83394, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 17s 344ms/step - loss: 0.9583 - accuracy: 0.6762 - val_loss: 0.8339 - val_accuracy: 0.7282\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.7038\n",
            "Epoch 24: val_loss did not improve from 0.83394\n",
            "50/50 [==============================] - 14s 278ms/step - loss: 0.8966 - accuracy: 0.7038 - val_loss: 0.8429 - val_accuracy: 0.7282\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8733 - accuracy: 0.7000\n",
            "Epoch 25: val_loss did not improve from 0.83394\n",
            "50/50 [==============================] - 14s 277ms/step - loss: 0.8733 - accuracy: 0.7000 - val_loss: 0.8400 - val_accuracy: 0.7767\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8634 - accuracy: 0.7063\n",
            "Epoch 26: val_loss did not improve from 0.83394\n",
            "50/50 [==============================] - 14s 280ms/step - loss: 0.8634 - accuracy: 0.7063 - val_loss: 0.8499 - val_accuracy: 0.7282\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.7163\n",
            "Epoch 27: val_loss improved from 0.83394 to 0.81078, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 314ms/step - loss: 0.8434 - accuracy: 0.7163 - val_loss: 0.8108 - val_accuracy: 0.7379\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8169 - accuracy: 0.7275\n",
            "Epoch 28: val_loss improved from 0.81078 to 0.78338, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 305ms/step - loss: 0.8169 - accuracy: 0.7275 - val_loss: 0.7834 - val_accuracy: 0.7621\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7951 - accuracy: 0.7400\n",
            "Epoch 29: val_loss did not improve from 0.78338\n",
            "50/50 [==============================] - 14s 288ms/step - loss: 0.7951 - accuracy: 0.7400 - val_loss: 0.8313 - val_accuracy: 0.7427\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.7412\n",
            "Epoch 30: val_loss improved from 0.78338 to 0.74415, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 18s 369ms/step - loss: 0.7650 - accuracy: 0.7412 - val_loss: 0.7441 - val_accuracy: 0.7670\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.7650\n",
            "Epoch 31: val_loss did not improve from 0.74415\n",
            "50/50 [==============================] - 13s 269ms/step - loss: 0.7307 - accuracy: 0.7650 - val_loss: 0.7445 - val_accuracy: 0.7573\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.7588\n",
            "Epoch 32: val_loss improved from 0.74415 to 0.71582, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 309ms/step - loss: 0.7480 - accuracy: 0.7588 - val_loss: 0.7158 - val_accuracy: 0.7524\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7862\n",
            "Epoch 33: val_loss improved from 0.71582 to 0.70642, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 19s 384ms/step - loss: 0.6798 - accuracy: 0.7862 - val_loss: 0.7064 - val_accuracy: 0.7864\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.7800\n",
            "Epoch 34: val_loss did not improve from 0.70642\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.6426 - accuracy: 0.7800 - val_loss: 0.7914 - val_accuracy: 0.7427\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.7713\n",
            "Epoch 35: val_loss did not improve from 0.70642\n",
            "50/50 [==============================] - 14s 273ms/step - loss: 0.6773 - accuracy: 0.7713 - val_loss: 0.7158 - val_accuracy: 0.7379\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6499 - accuracy: 0.7812\n",
            "Epoch 36: val_loss improved from 0.70642 to 0.70414, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 311ms/step - loss: 0.6499 - accuracy: 0.7812 - val_loss: 0.7041 - val_accuracy: 0.7816\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.7713\n",
            "Epoch 37: val_loss improved from 0.70414 to 0.67888, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 310ms/step - loss: 0.6132 - accuracy: 0.7713 - val_loss: 0.6789 - val_accuracy: 0.8058\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.8062\n",
            "Epoch 38: val_loss did not improve from 0.67888\n",
            "50/50 [==============================] - 14s 273ms/step - loss: 0.5858 - accuracy: 0.8062 - val_loss: 0.7101 - val_accuracy: 0.7670\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.8012\n",
            "Epoch 39: val_loss improved from 0.67888 to 0.65930, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 317ms/step - loss: 0.5777 - accuracy: 0.8012 - val_loss: 0.6593 - val_accuracy: 0.7913\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8025\n",
            "Epoch 40: val_loss did not improve from 0.65930\n",
            "50/50 [==============================] - 13s 269ms/step - loss: 0.5608 - accuracy: 0.8025 - val_loss: 0.6615 - val_accuracy: 0.7816\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.8012\n",
            "Epoch 41: val_loss improved from 0.65930 to 0.65096, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 17s 334ms/step - loss: 0.5433 - accuracy: 0.8012 - val_loss: 0.6510 - val_accuracy: 0.7961\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.8000\n",
            "Epoch 42: val_loss did not improve from 0.65096\n",
            "50/50 [==============================] - 14s 275ms/step - loss: 0.5504 - accuracy: 0.8000 - val_loss: 0.6674 - val_accuracy: 0.7961\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.8363\n",
            "Epoch 43: val_loss improved from 0.65096 to 0.63859, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 15s 309ms/step - loss: 0.5143 - accuracy: 0.8363 - val_loss: 0.6386 - val_accuracy: 0.8010\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4845 - accuracy: 0.8338\n",
            "Epoch 44: val_loss improved from 0.63859 to 0.63183, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 316ms/step - loss: 0.4845 - accuracy: 0.8338 - val_loss: 0.6318 - val_accuracy: 0.8058\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.8250\n",
            "Epoch 45: val_loss did not improve from 0.63183\n",
            "50/50 [==============================] - 14s 276ms/step - loss: 0.5073 - accuracy: 0.8250 - val_loss: 0.6484 - val_accuracy: 0.7913\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.8450\n",
            "Epoch 46: val_loss improved from 0.63183 to 0.59021, saving model to CNN_best_model.h5\n",
            "50/50 [==============================] - 16s 322ms/step - loss: 0.4816 - accuracy: 0.8450 - val_loss: 0.5902 - val_accuracy: 0.8252\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.8363\n",
            "Epoch 47: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 273ms/step - loss: 0.4931 - accuracy: 0.8363 - val_loss: 0.6484 - val_accuracy: 0.8058\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8675\n",
            "Epoch 48: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 275ms/step - loss: 0.4196 - accuracy: 0.8675 - val_loss: 0.6466 - val_accuracy: 0.7913\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.8475\n",
            "Epoch 49: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 273ms/step - loss: 0.4696 - accuracy: 0.8475 - val_loss: 0.5959 - val_accuracy: 0.8155\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8725\n",
            "Epoch 50: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 278ms/step - loss: 0.3894 - accuracy: 0.8725 - val_loss: 0.6528 - val_accuracy: 0.7864\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8625\n",
            "Epoch 51: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 275ms/step - loss: 0.4430 - accuracy: 0.8625 - val_loss: 0.6341 - val_accuracy: 0.8010\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3976 - accuracy: 0.8737\n",
            "Epoch 52: val_loss did not improve from 0.59021\n",
            "50/50 [==============================] - 14s 276ms/step - loss: 0.3976 - accuracy: 0.8737 - val_loss: 0.6027 - val_accuracy: 0.8010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7988080c8ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "multi_classifier = Sequential()\n",
        "\n",
        "multi_classifier.add(Conv2D(32, (3, 3), input_shape = (224, 224, 3), activation = 'relu'))\n",
        "multi_classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "multi_classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
        "multi_classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "multi_classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
        "multi_classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "multi_classifier.add(Conv2D(256, (3, 3), activation = 'relu'))\n",
        "multi_classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "multi_classifier.add(Dropout(0.2))\n",
        "multi_classifier.add(Flatten())\n",
        "\n",
        "multi_classifier.add(Dense(units=1024, activation='relu'))\n",
        "multi_classifier.add(Dropout(0.2))\n",
        "multi_classifier.add(Dense(units=512, activation='relu'))\n",
        "multi_classifier.add(Dropout(0.2))\n",
        "multi_classifier.add(Dense(units=256, activation='relu'))\n",
        "\n",
        "multi_classifier.add(Dense(units = 10, activation = 'softmax'))\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate = 0.00002)\n",
        "multi_classifier.compile(optimizer = optimizer,loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "multi_training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/face_recognition/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 16,\n",
        "                                                 class_mode = 'categorical')\n",
        "multi_val_set = test_datagen.flow_from_directory('/content/drive/MyDrive/face_recognition/val',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 16,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=6)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('CNN_best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "\n",
        "multi_classifier.fit(x = multi_training_set, validation_data = multi_val_set, epochs = 100, callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG16 model"
      ],
      "metadata": {
        "id": "t-SxyxJ5-blB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "num_classes = 10\n",
        "input_size = (224, 224)\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(input_size[0], input_size[1], 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/face_recognition/train',\n",
        "    target_size=input_size,\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/face_recognition/val',\n",
        "    target_size=input_size,\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('VGG16_best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smLgOb4K-ai3",
        "outputId": "359e4b75-7010-4561-e7b7-a1f3255c9157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Found 800 images belonging to 10 classes.\n",
            "Found 206 images belonging to 10 classes.\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2119 - accuracy: 0.3862\n",
            "Epoch 1: val_loss improved from inf to 1.18450, saving model to VGG16_best_model.h5\n",
            "50/50 [==============================] - 20s 286ms/step - loss: 2.2119 - accuracy: 0.3862 - val_loss: 1.1845 - val_accuracy: 0.6068\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9024 - accuracy: 0.6862\n",
            "Epoch 2: val_loss improved from 1.18450 to 0.98965, saving model to VGG16_best_model.h5\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.9024 - accuracy: 0.6862 - val_loss: 0.9896 - val_accuracy: 0.6893\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7538\n",
            "Epoch 3: val_loss did not improve from 0.98965\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.6968 - accuracy: 0.7538 - val_loss: 1.0470 - val_accuracy: 0.6748\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.7613\n",
            "Epoch 4: val_loss improved from 0.98965 to 0.66347, saving model to VGG16_best_model.h5\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.7272 - accuracy: 0.7613 - val_loss: 0.6635 - val_accuracy: 0.7718\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.8238\n",
            "Epoch 5: val_loss did not improve from 0.66347\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.4868 - accuracy: 0.8238 - val_loss: 0.8426 - val_accuracy: 0.6942\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8600\n",
            "Epoch 6: val_loss did not improve from 0.66347\n",
            "50/50 [==============================] - 14s 280ms/step - loss: 0.4248 - accuracy: 0.8600 - val_loss: 0.8029 - val_accuracy: 0.7379\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8500\n",
            "Epoch 7: val_loss did not improve from 0.66347\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.4129 - accuracy: 0.8500 - val_loss: 0.8302 - val_accuracy: 0.7379\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8612\n",
            "Epoch 8: val_loss did not improve from 0.66347\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.3807 - accuracy: 0.8612 - val_loss: 0.7580 - val_accuracy: 0.7621\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8900\n",
            "Epoch 9: val_loss did not improve from 0.66347\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.3191 - accuracy: 0.8900 - val_loss: 1.1034 - val_accuracy: 0.6796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x798a7ebdf970>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGGFace model"
      ],
      "metadata": {
        "id": "Mt5v7qzpZr7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JbFqy_lzrKo",
        "outputId": "3c96d0a7-f6c9-4faf-c1ff-fca8350af85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/yaledhlab/vggface.git\n",
            "  Cloning https://github.com/yaledhlab/vggface.git to /tmp/pip-req-build-51i_cfx1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/yaledhlab/vggface.git /tmp/pip-req-build-51i_cfx1\n",
            "  Resolved https://github.com/yaledhlab/vggface.git to commit b76539b7588bca69b0030ad7e4f985f877dc7c0a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.11.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (3.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (9.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (2.15.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (6.0.1)\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/yaledhlab/vggface.git\n",
        "!pip install keras_applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI6KdHKTzYvv",
        "outputId": "a4676bff-1ec9-4efa-a8df-4f8079b074f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 10 classes.\n",
            "Found 206 images belonging to 10 classes.\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.0141 - accuracy: 0.3812\n",
            "Epoch 1: val_loss improved from inf to 3.60451, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 16s 570ms/step - loss: 18.0141 - accuracy: 0.3812 - val_loss: 3.6045 - val_accuracy: 0.8107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.6647 - accuracy: 0.8338\n",
            "Epoch 2: val_loss improved from 3.60451 to 0.95710, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 568ms/step - loss: 2.6647 - accuracy: 0.8338 - val_loss: 0.9571 - val_accuracy: 0.9320\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.1046 - accuracy: 0.9237\n",
            "Epoch 3: val_loss did not improve from 0.95710\n",
            "25/25 [==============================] - 14s 560ms/step - loss: 1.1046 - accuracy: 0.9237 - val_loss: 0.9964 - val_accuracy: 0.9369\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7576 - accuracy: 0.9413\n",
            "Epoch 4: val_loss improved from 0.95710 to 0.63289, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 555ms/step - loss: 0.7576 - accuracy: 0.9413 - val_loss: 0.6329 - val_accuracy: 0.9515\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.9600\n",
            "Epoch 5: val_loss improved from 0.63289 to 0.58645, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 563ms/step - loss: 0.4872 - accuracy: 0.9600 - val_loss: 0.5865 - val_accuracy: 0.9612\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.9613\n",
            "Epoch 6: val_loss improved from 0.58645 to 0.54653, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 16s 624ms/step - loss: 0.3777 - accuracy: 0.9613 - val_loss: 0.5465 - val_accuracy: 0.9660\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.9762\n",
            "Epoch 7: val_loss improved from 0.54653 to 0.28545, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 553ms/step - loss: 0.2741 - accuracy: 0.9762 - val_loss: 0.2855 - val_accuracy: 0.9660\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9775\n",
            "Epoch 8: val_loss did not improve from 0.28545\n",
            "25/25 [==============================] - 14s 540ms/step - loss: 0.1903 - accuracy: 0.9775 - val_loss: 0.3772 - val_accuracy: 0.9757\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9725\n",
            "Epoch 9: val_loss did not improve from 0.28545\n",
            "25/25 [==============================] - 13s 525ms/step - loss: 0.1873 - accuracy: 0.9725 - val_loss: 0.3358 - val_accuracy: 0.9709\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9812\n",
            "Epoch 10: val_loss did not improve from 0.28545\n",
            "25/25 [==============================] - 13s 524ms/step - loss: 0.1710 - accuracy: 0.9812 - val_loss: 0.4387 - val_accuracy: 0.9709\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9762\n",
            "Epoch 11: val_loss improved from 0.28545 to 0.25317, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 554ms/step - loss: 0.1221 - accuracy: 0.9762 - val_loss: 0.2532 - val_accuracy: 0.9757\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9787\n",
            "Epoch 12: val_loss improved from 0.25317 to 0.24168, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 571ms/step - loss: 0.1678 - accuracy: 0.9787 - val_loss: 0.2417 - val_accuracy: 0.9854\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9825\n",
            "Epoch 13: val_loss did not improve from 0.24168\n",
            "25/25 [==============================] - 14s 557ms/step - loss: 0.1976 - accuracy: 0.9825 - val_loss: 0.4825 - val_accuracy: 0.9563\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9825\n",
            "Epoch 14: val_loss did not improve from 0.24168\n",
            "25/25 [==============================] - 14s 552ms/step - loss: 0.1450 - accuracy: 0.9825 - val_loss: 0.3520 - val_accuracy: 0.9660\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9837\n",
            "Epoch 15: val_loss did not improve from 0.24168\n",
            "25/25 [==============================] - 14s 543ms/step - loss: 0.1673 - accuracy: 0.9837 - val_loss: 0.2591 - val_accuracy: 0.9806\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9812\n",
            "Epoch 16: val_loss improved from 0.24168 to 0.11224, saving model to VGGFace_best_model.h5\n",
            "25/25 [==============================] - 14s 544ms/step - loss: 0.1759 - accuracy: 0.9812 - val_loss: 0.1122 - val_accuracy: 0.9903\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9925\n",
            "Epoch 17: val_loss did not improve from 0.11224\n",
            "25/25 [==============================] - 14s 550ms/step - loss: 0.0706 - accuracy: 0.9925 - val_loss: 0.2748 - val_accuracy: 0.9854\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9937\n",
            "Epoch 18: val_loss did not improve from 0.11224\n",
            "25/25 [==============================] - 15s 609ms/step - loss: 0.0410 - accuracy: 0.9937 - val_loss: 0.1608 - val_accuracy: 0.9806\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9900\n",
            "Epoch 19: val_loss did not improve from 0.11224\n",
            "25/25 [==============================] - 14s 550ms/step - loss: 0.0761 - accuracy: 0.9900 - val_loss: 0.1831 - val_accuracy: 0.9903\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9875\n",
            "Epoch 20: val_loss did not improve from 0.11224\n",
            "25/25 [==============================] - 14s 554ms/step - loss: 0.0660 - accuracy: 0.9875 - val_loss: 0.1172 - val_accuracy: 0.9903\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9887\n",
            "Epoch 21: val_loss did not improve from 0.11224\n",
            "25/25 [==============================] - 14s 548ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.1959 - val_accuracy: 0.9854\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79881c2e6650>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "num_classes = 10\n",
        "input_size = (224, 224)\n",
        "\n",
        "base_model = VGGFace(include_top=False, input_shape=(224, 224, 3), pooling='max')\n",
        "x = base_model.output\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/face_recognition/train',\n",
        "    target_size=input_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/face_recognition/val',\n",
        "    target_size=input_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('VGGFace_best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B3-NVwwdducV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Yolo model za prepoznavanje lica"
      ],
      "metadata": {
        "id": "0GvzFSGsdzI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics"
      ],
      "metadata": {
        "id": "_hvRud0jl7XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO\n",
        "yolo_results = []\n",
        "output_root = '/content/drive/MyDrive/face_recognition'\n",
        "model_yolo = YOLO('/content/drive/MyDrive/yolov8n-cls.pt')\n",
        "results = model_yolo.train(data=output_root, epochs=50, patience=5, imgsz=224, batch=16)"
      ],
      "metadata": {
        "id": "UGnNTHfFd1Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629ff4ea-4569-485c-ce99-96dca9686fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=/content/drive/MyDrive/yolov8n-cls.pt, data=/content/drive/MyDrive/face_recognition, epochs=50, time=None, patience=5, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/face_recognition/train... found 800 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/face_recognition/val... found 206 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "Overriding model.yaml nc=1000 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n",
            "YOLOv8n-cls summary: 99 layers, 1451098 parameters, 1451098 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/face_recognition/train... 800 images, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/face_recognition/val... 206 images, 0 corrupt: 100%|██████████| 206/206 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50     0.392G       2.32         16        224:   2%|▏         | 1/50 [00:05<04:18,  5.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 755k/755k [00:00<00:00, 20.5MB/s]\n",
            "       1/50     0.396G      2.199         16        224: 100%|██████████| 50/50 [02:14<00:00,  2.69s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:15<00:00,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.456       0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50     0.396G       1.78         16        224: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.587      0.956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50     0.396G      1.239         16        224: 100%|██████████| 50/50 [00:07<00:00,  7.12it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.738       0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50     0.396G     0.8615         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.09it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.845      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50     0.396G     0.6229         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.845       0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50     0.396G     0.4778         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.26it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.869      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50     0.396G      0.369         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.15it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  7.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.908          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50     0.396G     0.2884         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.28it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  6.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.913      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50     0.396G     0.2455         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.33it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.942          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50     0.396G     0.1926         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.00it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.927      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50     0.396G     0.2275         16        224: 100%|██████████| 50/50 [00:08<00:00,  5.94it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  9.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.927      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50     0.396G     0.1936         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.26it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951       0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50     0.396G     0.1568         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.06it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50     0.396G     0.1751         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.04it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  7.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50     0.394G     0.1942         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.42it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50     0.394G     0.1254         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.47it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50     0.396G     0.1452         16        224: 100%|██████████| 50/50 [00:08<00:00,  5.96it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.947      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50     0.396G     0.1028         16        224: 100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  6.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50     0.396G     0.1017         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.60it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  9.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50     0.396G     0.1052         16        224: 100%|██████████| 50/50 [00:08<00:00,  5.98it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50     0.396G    0.09708         16        224: 100%|██████████| 50/50 [00:07<00:00,  7.07it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50     0.396G     0.0761         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.76it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50     0.394G     0.1334         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.05it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  7.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50     0.396G     0.0857         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.95it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.971      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50     0.396G    0.07737         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.39it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.961          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50     0.396G    0.06899         16        224: 100%|██████████| 50/50 [00:08<00:00,  5.96it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  9.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50     0.396G    0.07501         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.981      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50     0.396G     0.0946         16        224: 100%|██████████| 50/50 [00:06<00:00,  7.46it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.961          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50     0.396G    0.06799         16        224: 100%|██████████| 50/50 [00:08<00:00,  6.10it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.961          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50     0.396G    0.06105         16        224: 100%|██████████| 50/50 [00:07<00:00,  6.43it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.971          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50     0.396G    0.07129         16        224: 100%|██████████| 50/50 [00:07<00:00,  7.12it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50     0.396G    0.05963         16        224: 100%|██████████| 50/50 [00:08<00:00,  5.89it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:00<00:00,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.961          1\n",
            "Stopping training early as no improvement observed in last 5 epochs. Best results observed at epoch 27, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "32 epochs completed in 0.121 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1447690 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/face_recognition/train... found 800 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/face_recognition/val... found 206 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|██████████| 7/7 [00:01<00:00,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.981      0.995\n",
            "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model_yolo.val()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW_GYS1iSlwN",
        "outputId": "ec142785-6622-4448-81d7-cb7ddbb872f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1447690 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/face_recognition/train... found 800 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/face_recognition/val... found 206 images in 10 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/face_recognition/val... 206 images, 0 corrupt: 100%|██████████| 206/206 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 13/13 [00:03<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.981      0.995\n",
            "Speed: 0.2ms preprocess, 6.5ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Provjera modela na slici"
      ],
      "metadata": {
        "id": "84U7hGxpeNZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_yolo('Tom_Cruise_0008.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIARDICeoonh",
        "outputId": "6751b619-410d-4b19-8bb2-d4995d1fb793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/Tom_Cruise_0008.jpg: 224x224 Tom_Cruise 1.00, Brad_Pitt 0.00, Nedim_Omeragic 0.00, Kate_Winslet 0.00, Natalie_Portman 0.00, 4.3ms\n",
            "Speed: 3.2ms preprocess, 4.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Real-time photo recognition"
      ],
      "metadata": {
        "id": "kVT8Bj-aaD8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArrMeGeslB3k",
        "outputId": "6e93c11c-5065-4cf7-8a6e-7a8dabcb393c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWsNiGO5lEb2",
        "outputId": "bda499c2-dd4a-40a4-b36a-9ddeb42b133a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenCV-Python Version 4.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "print(\"OpenCV-Python Version {}\".format(cv2.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hh-gL3IlKcB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLFVaOoxxvjR",
        "outputId": "c1842354-a48d-4a89-a155-8e66c4765607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 26.3/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "yolo_model = YOLO(\"/content/drive/MyDrive/yolov8n-face.pt\")\n",
        "model_yolo_cls = YOLO(\"/content/drive/MyDrive/yoloclsbest.pt\")"
      ],
      "metadata": {
        "id": "zXdA85CBx0bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d80c1a1-46e6-4473-9d46-33a2f85bb303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ /content/drive/MyDrive/yolov8n-face.pt appears to require 'omegaconf', which is not in ultralytics requirements.\n",
            "AutoInstall will run now for 'omegaconf' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['omegaconf'] not found, attempting AutoUpdate...\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 3.8 MB/s eta 0:00:00\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 14.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d2bbbdf45804b8a778fc19813775a176783ef3a497edcd33e4ab114f651df30c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkef02ee/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.9s, installed 1 package: ['omegaconf']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "Uuv3xVBEx6MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  results = yolo_model(filename, verbose=False)\n",
        "\n",
        "  boxes = results[0].boxes\n",
        "  img = cv2.imread(filename)\n",
        "\n",
        "  for box in boxes:\n",
        "    top_left_x = int(box.xyxy.tolist()[0][0])\n",
        "    top_left_y = int(box.xyxy.tolist()[0][1])\n",
        "    bottom_right_x = int(box.xyxy.tolist()[0][2])\n",
        "    bottom_right_y = int(box.xyxy.tolist()[0][3])\n",
        "    cropped_image = img[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
        "    image2 = cv2.resize(cropped_image, (224, 224))\n",
        "    cv2_imshow(image2)\n",
        "    img_array = image.img_to_array(image2)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= .255\n",
        "\n",
        "    #Ispod se koristi YOLO model za predikciju, ostali modeli su zakomentarisani\n",
        "    #Yolo\n",
        "    predictions2 = model_yolo_cls(image2, verbose=False)\n",
        "    #NasaCNN\n",
        "    #predictions2 = multi_classifier.predict(img_array)\n",
        "    #VGGFace\n",
        "    #predictions2 = model.predict(img_array, verbose=False)\n",
        "\n",
        "    #Index za ostale modele\n",
        "    #predicted_class_index2 = np.argmax(predictions2)\n",
        "\n",
        "    #Index za yolo\n",
        "    predicted_class_index2 = predictions2[0].probs.top1\n",
        "\n",
        "    predicted_label = people['name'][predicted_class_index2]\n",
        "    #Confidence za ostale modele\n",
        "    #confidence = np.max(predictions2)\n",
        "\n",
        "    #Confidence za yolo\n",
        "    confidence = predictions2[0].probs.data[predicted_class_index2]\n",
        "\n",
        "    if(confidence < 0.7):\n",
        "      predicted_label = \"Unknown person\"\n",
        "\n",
        "    print(\"Predicted Label:\", predicted_label, confidence)\n",
        "\n",
        "except Exception as err:\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "adVUOsPkyEPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Real-time video recognition"
      ],
      "metadata": {
        "id": "OlBFVt90BkQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "metadata": {
        "id": "OlvnAQfVDg3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pomocne funkcije\n"
      ],
      "metadata": {
        "id": "dmZ-0_bmcCSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"Status:\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '' +\n",
        "          'When finished, click here or on the video to stop this demo';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "mLTOfASCyERt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Real-time prepoznavanje"
      ],
      "metadata": {
        "id": "jcmKRU1vcGoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = yolo_model(rgb, verbose=False)\n",
        "    boxes = results[0].boxes\n",
        "    for box in boxes:\n",
        "      top_left_x = int(box.xyxy.tolist()[0][0])\n",
        "      top_left_y = int(box.xyxy.tolist()[0][1])\n",
        "      bottom_right_x = int(box.xyxy.tolist()[0][2])\n",
        "      bottom_right_y = int(box.xyxy.tolist()[0][3])\n",
        "      cropped_image2 = rgb[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
        "      image3 = cv2.resize(cropped_image2, (224, 224))\n",
        "      img_array2 = image.img_to_array(image3)\n",
        "      img_array2 = np.expand_dims(img_array2, axis=0)\n",
        "      img_array2 /= .255\n",
        "\n",
        "      #Yolo\n",
        "      predictions2 = model_yolo_cls(image3, verbose=False)\n",
        "      #NasaCNN\n",
        "      #predictions2 = multi_classifier.predict(img_array)\n",
        "      #VGGFace\n",
        "      #predictions2 = model.predict(img_array2, verbose=False)\n",
        "      #multi_classifier.predict(image2)\n",
        "      #print(predictions2)\n",
        "\n",
        "      #Index za ostale modele\n",
        "      #predicted_class_index2 = np.argmax(predictions2)\n",
        "\n",
        "      #Index za yolo\n",
        "      predicted_class_index2 = predictions2[0].probs.top1\n",
        "\n",
        "      predicted_label2 = people['name'][predicted_class_index2]\n",
        "      #Confidence za ostale modele\n",
        "      #confidence = np.max(predictions2)\n",
        "\n",
        "      #Confidence za yolo\n",
        "      confidence = predictions2[0].probs.data[predicted_class_index2]\n",
        "\n",
        "\n",
        "      if(confidence > 0.7):\n",
        "        bbox_array = cv2.rectangle(bbox_array, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (50, 200, 129), 2)\n",
        "        bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(predicted_label2, predictions2[0].probs.data[predicted_class_index2]),\n",
        "                        (top_left_x, top_left_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        (50, 200, 129), 2)\n",
        "      else:\n",
        "        bbox_array = cv2.rectangle(bbox_array, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (50, 200, 129), 2)\n",
        "        bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format('Unknown person', 1),\n",
        "                        (top_left_x, top_left_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        (50, 200, 129), 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox = bbox_bytes\n"
      ],
      "metadata": {
        "id": "IG-Tn72XBxPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "19be83b7-b7ea-44c6-9a2a-b9f54941a484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"Status:\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '' +\n",
              "          'When finished, click here or on the video to stop this demo';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640;\n",
              "      captureCanvas.height = 480;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8HXij17oYHTa",
        "o20wxy7PYMi5",
        "VFrvxpLlYXq5",
        "D_gGM_n9Yfea",
        "5XC7SeUGYl7w",
        "4EMZVB7fYpoa",
        "_ZGHfguQphiT",
        "phyYYzjXYt_e",
        "t-SxyxJ5-blB",
        "Mt5v7qzpZr7n",
        "kVT8Bj-aaD8v"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}